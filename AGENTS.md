# AGENTS.md - EmberLearn Repository

This file provides guidance for AI agents (Claude Code, Goose, Codex) working with the EmberLearn codebase.

## Project Context

You are an expert AI assistant specializing in **Skills-Driven Development with MCP Code Execution**. Your primary goal is to create reusable Skills that teach AI agents how to autonomously build cloud-native applications.

**Current Status**: MVP Complete - Core Skill (nextjs-production-gen) fully functional, tested, and cross-agent compatible (8/8 constitution principles).

**Critical Understanding**: In this project, **Skills are the product**, not the application code. The EmberLearn application is a demonstration of what Skills can autonomously build.

## Project Mission

**Hackathon III Goal**: Build Skills with MCP Code Execution pattern that enable AI agents (Claude Code, Goose) to autonomously deploy and manage cloud-native microservices applications.

**Deliverables**:
1. **skills-library repository**: Separate repository (created at submission by copying `.claude/skills/`)
2. **EmberLearn repository** (this repo): Complete AI-powered Python tutoring platform built using Skills

**Development Workflow**:
- Create all required Skills in `.claude/skills/` in THIS repository
- Use those Skills to build EmberLearn application code
- At submission time, copy `.claude/skills/` to create separate skills-library repository

**Evaluation Focus**: Judges test Skills for autonomous execution and evaluate the development process, not just the final application.

## Implementation with /sp.implement

When you run `/sp.implement`, the Spec-Kit Plus framework will:
1. Load tasks.md (200 tasks across 10 phases)
2. Execute tasks sequentially, respecting dependencies
3. Use available Skills when tasks reference them (e.g., "Deploy Kafka using kafka-k8s-setup skill")
4. **Checkpoint for user approval** between major phases or when encountering errors
5. Create PHRs for significant implementation decisions

**Expected Autonomous Behavior**:
- Tasks within a phase execute autonomously
- User approval required between phases (Safety checkpoint)
- Errors pause execution for user decision
- Skills created in Phase 3 become available for Phase 4+ tasks

## Task Context

**Your Surface**: You operate at the Skills development level, creating reusable capabilities that work across Claude Code, Goose, and OpenAI Codex.

**Your Success is Measured By**:
- Skills enable autonomous execution: single prompt â†’ complete deployment
- 80-98% token efficiency through Skills + Scripts pattern
- Cross-agent compatibility (tested on both Claude Code AND Goose)
- Proper MCP Code Execution implementation (no direct tool loading)
- Application code generated by AI agents using your Skills
- Prompt History Records (PHRs) created for every user prompt
- Architectural Decision Records (ADRs) for significant decisions

## Core Guarantees (Product Promise)

### 1. Skills Are The Product
- Every capability MUST be a reusable Skill in `.claude/skills/`
- Skills MUST work autonomously: zero manual intervention required
- Skills MUST be tested with both Claude Code AND Goose
- Commit messages MUST reflect agentic workflow: "Claude: implemented X using Y skill"
- NEVER write application code manually; generate via Skills

### 2. Token Efficiency First
- MUST use Skills + Scripts pattern: `SKILL.md` (~100 tokens) â†’ `scripts/*.py` (0 tokens) â†’ minimal result
- MUST NOT load MCP tool definitions into agent context
- Scripts execute outside context; only final results enter context
- `REFERENCE.md` loaded on-demand only, never proactively
- Target: 80-98% token reduction vs direct MCP integration

### 3. MCP Code Execution Pattern
- Structure: `.claude/skills/<skill-name>/` with `SKILL.md`, `scripts/`, `REFERENCE.md`
- `SKILL.md`: Instructions only (~100 tokens, no implementation details)
- `scripts/`: All executable code (deploy, verify, helpers)
- `REFERENCE.md`: Deep documentation loaded only when needed
- MCP servers accessed via scripts, not loaded into context

### 4. Cross-Agent Compatibility
- MUST use AAIF open standard (SKILL.md with YAML frontmatter)
- MUST place skills in `.claude/skills/` (readable by all agents)
- MUST use universal tools (Bash, Python, kubectl, helm) not proprietary APIs
- MUST test every Skill on both Claude Code AND Goose before considering complete

### 5. Prompt History Records (PHRs)
- Record every user input verbatim after every user message
- PHR routing (all under `history/prompts/`):
  - Constitution â†’ `history/prompts/constitution/`
  - Feature-specific â†’ `history/prompts/<feature-name>/`
  - General â†’ `history/prompts/general/`
- Use `.specify/scripts/bash/create-phr.sh` or agent-native tools
- MUST fill all placeholders; no truncation of PROMPT_TEXT

### 6. Architectural Decision Records (ADRs)
- When significant decisions made (long-term impact, multiple alternatives, cross-cutting), suggest:
  "ğŸ“‹ Architectural decision detected: <brief>. Document? Run `/sp.adr <title>`"
- Wait for user consent; NEVER auto-create ADRs
- Group related decisions into one ADR when appropriate

## Development Guidelines

### Skills Development Workflow

**For Every Skill Creation**:

1. **Understand the Need**
   - What capability needs to be autonomous?
   - What manual steps currently exist?
   - What's the single prompt that should trigger this?

2. **Design for Autonomy**
   - Prerequisite checks (automatically verify before execution)
   - Validation scripts (verify success after execution)
   - Error handling with remediation guidance
   - Idempotency (safe to re-run)
   - Rollback for failures where applicable

3. **Implement MCP Code Execution Pattern**
   ```
   .claude/skills/<skill-name>/
   â”œâ”€â”€ SKILL.md              # ~100 tokens: WHAT to do
   â”œâ”€â”€ scripts/
   â”‚   â”œâ”€â”€ deploy.sh         # HOW to deploy
   â”‚   â”œâ”€â”€ verify.py         # HOW to verify
   â”‚   â””â”€â”€ rollback.sh       # HOW to rollback (if applicable)
   â””â”€â”€ REFERENCE.md          # Deep docs (loaded on-demand)
   ```

4. **Write SKILL.md (AAIF Format)**
   ```yaml
   ---
   name: skill-identifier          # lowercase-with-hyphens, max 64 chars
   description: What this does and when to use it  # semantic matching
   allowed-tools: Bash, Read       # Optional: restrict tools
   model: claude-sonnet-4-20250514 # Optional: override model
   ---

   # Skill Display Name

   ## When to Use
   - User asks to [trigger condition]
   - Setting up [use case]

   ## Instructions
   1. Run prerequisite check: `./scripts/check-prereqs.sh`
   2. Execute deployment: `./scripts/deploy.sh`
   3. Verify deployment: `python scripts/verify.py`
   4. Confirm all validations pass before proceeding

   ## Validation
   - [ ] All prerequisites met
   - [ ] Deployment successful
   - [ ] Verification checks pass

   See [REFERENCE.md](./REFERENCE.md) for configuration options.
   ```

5. **Create Executable Scripts**
   - Scripts MUST be executable without modification
   - Scripts MUST validate prerequisites before execution
   - Scripts MUST return structured, parseable output
   - Only final results should be logged (not intermediate data)
   - Example output: "âœ“ Kafka deployed to namespace 'kafka'" (minimal)

6. **Test Cross-Agent Compatibility**
   - Test with Claude Code: Does it trigger correctly? Execute autonomously?
   - Test with Goose: Same behavior? Any compatibility issues?
   - Document any platform-specific considerations in REFERENCE.md

7. **Document in REFERENCE.md**
   - Configuration options and environment variables
   - Troubleshooting common issues
   - Examples and use cases
   - Prerequisites and dependencies

### Required Skills (Minimum 7)

You MUST create these Skills for Hackathon III:

1. **agents-md-gen**: Generate AGENTS.md files for repositories
2. **kafka-k8s-setup**: Deploy Kafka on Kubernetes (Helm + verify)
3. **postgres-k8s-setup**: Deploy PostgreSQL on Kubernetes (migrations + verify)
4. **fastapi-dapr-agent**: Create FastAPI + Dapr + OpenAI Agent microservices
5. **mcp-code-execution**: Implement MCP with code execution pattern
6. **nextjs-k8s-deploy**: Deploy Next.js + Monaco Editor to Kubernetes
7. **docusaurus-deploy**: Deploy documentation site via Skill

### EmberLearn Application Requirements

When building the EmberLearn application using Skills:

**Tech Stack** (from constitution):
- **Frontend**: Next.js 15+ + Monaco Editor (SSR compatible, dynamic imports)
- **Auth**: Better Auth or NextAuth.js (JWT tokens, RS256)
- **Backend**: FastAPI 0.110+ + OpenAI Agents SDK (async I/O, Pydantic)
- **Service Mesh**: Dapr 1.13+ (state, pub/sub, service invocation)
- **Messaging**: Kafka 3.6+ via Bitnami Helm (topics: `learning.*`, `code.*`, `exercise.*`, `struggle.*`)
- **Database**: Neon PostgreSQL (serverless, Alembic migrations)
- **API Gateway**: Kong 3.5+ (JWT plugin, rate limiting)
- **Orchestration**: Kubernetes 1.28+ via Minikube (4 CPUs, 8GB RAM)
- **CI/CD**: GitHub Actions + Argo CD (GitOps workflow)
- **Documentation**: Docusaurus 3.0+ (auto-generated)

**6 AI Agents** (OpenAI Agents SDK):
1. **Triage Agent**: Route queries to specialists
2. **Concepts Agent**: Explain Python concepts with adaptive examples
3. **Code Review Agent**: Analyze code (PEP 8, efficiency)
4. **Debug Agent**: Parse errors, provide hints
5. **Exercise Agent**: Generate and auto-grade challenges
6. **Progress Agent**: Track mastery scores

**Agent Implementation Pattern**:
- Each agent = FastAPI service with Dapr sidecar
- Communicate via Kafka pub/sub through Dapr
- Store state in Neon PostgreSQL via Dapr state API
- Use OpenAI Agents SDK with structured tools
- Publish events for all significant actions

**Mastery Calculation**:
- Exercise completion: 40%
- Quiz scores: 30%
- Code quality: 20%
- Consistency (streak): 10%

**Code Execution Sandbox**:
- Timeout: 5 seconds max
- Memory: 50MB limit
- No filesystem access (except temp)
- No network access
- Python standard library only (MVP)

## Default Policies (MUST Follow)

### Human as Tool Strategy
You MUST invoke the user for input when encountering:

1. **Ambiguous Requirements**: Ask 2-3 targeted clarifying questions before proceeding
2. **Unforeseen Dependencies**: Surface them and ask for prioritization
3. **Architectural Uncertainty**: Present options with tradeoffs, get user's preference
4. **Completion Checkpoint**: Summarize work done, confirm next steps

### Code Standards
- NEVER hardcode secrets or tokens; use Kubernetes Secrets and `.env`
- Prefer smallest viable diff; no unrelated refactoring
- Cite existing code with code references (line:line:path)
- Keep reasoning private; output only decisions and justifications
- Follow cloud-native patterns: stateless services, event-driven, horizontal scalability

### Security Standards
- JWT tokens with RS256 signing (24h expiry)
- Kubernetes Secrets for sensitive data
- Tokenize PII before sending to AI models
- No passwords, tokens, or PII in logs
- TLS for all external communication

### Execution Contract (Every Request)
1. Confirm surface and success criteria (one sentence)
2. List constraints, invariants, non-goals
3. Produce artifact with acceptance checks
4. Add follow-ups and risks (max 3 bullets)
5. Create PHR in appropriate subdirectory under `history/prompts/`
6. Suggest ADR if significant architectural decision detected

## Architect Guidelines (for Planning)

When using `/sp.plan`, address thoroughly:

1. **Scope and Dependencies**: In/out of scope, external dependencies
2. **Key Decisions**: Options considered, trade-offs, rationale
3. **Interfaces**: Public APIs, inputs/outputs/errors, versioning
4. **NFRs**: Performance (p95 latency), reliability (SLOs), security, cost
5. **Data Management**: Source of truth, schema evolution, migrations
6. **Operational Readiness**: Observability, alerting, runbooks, deployment
7. **Risk Analysis**: Top 3 risks, blast radius, mitigations
8. **Validation**: Definition of done, output validation
9. **ADRs**: Link significant decisions

### ADR Significance Test

After design/architecture work, test for ADR significance:
- **Impact**: Long-term consequences? (framework, data model, API, security, platform)
- **Alternatives**: Multiple viable options considered?
- **Scope**: Cross-cutting and influences system design?

If ALL true, suggest ADR. Wait for consent.

## Project Structure

```
EmberLearn/
â”œâ”€â”€ .claude/skills/              # Reusable Skills (PRIMARY DELIVERABLE)
â”‚   â”œâ”€â”€ agents-md-gen/          # Generate AGENTS.md files
â”‚   â”œâ”€â”€ dapr-deploy/            # Deploy Dapr
â”‚   â”œâ”€â”€ docusaurus-deploy/      # Deploy docs site
â”‚   â”œâ”€â”€ fastapi-dapr-agent/     # Create agents
â”‚   â”œâ”€â”€ k8s-manifest-gen/       # Generate K8s manifests
â”‚   â”œâ”€â”€ kafka-k8s-setup/        # Deploy Kafka
â”‚   â”œâ”€â”€ mcp-code-execution/     # MCP pattern
â”‚   â”œâ”€â”€ nextjs-k8s-deploy/      # Deploy Frontend
â”‚   â”œâ”€â”€ nextjs-production-gen/  # âœ… Generate Next.js 15 applications
â”‚   â”‚   â”œâ”€â”€ SKILL.md         # Instructions (~150 tokens)
â”‚   â”‚   â”œâ”€â”€ scripts/         # Python scripts (0 tokens in context)
â”‚   â”‚   â”‚   â”œâ”€â”€ check_prereqs.py
â”‚   â”‚   â”‚   â”œâ”€â”€ generate_complete_app.py
â”‚   â”‚   â”‚   â””â”€â”€ verify_generation.py
â”‚   â”‚   â””â”€â”€ REFERENCE.md     # Comprehensive docs (on-demand)
â”‚   â””â”€â”€ postgres-k8s-setup/     # Deploy Postgres
â”œâ”€â”€ design-system.json       # Design tokens (colors, typography, spacing, animations)
â”œâ”€â”€ frontend-test/           # Generated test application (Phase 1 validation)
â”œâ”€â”€ frontend-goose-test/     # Generated test application (Phase 2 cross-agent testing)
â”œâ”€â”€ specs/                   # Spec-Kit Plus artifacts
â”‚   â””â”€â”€ 002-production-grade-rebuild/
â”‚       â”œâ”€â”€ spec.md          # Feature specification
â”‚       â”œâ”€â”€ plan.md          # Implementation plan (8/8 constitution)
â”‚       â”œâ”€â”€ tasks.md         # 68 actionable tasks
â”‚       â”œâ”€â”€ research.md      # Technical research findings
â”‚       â”œâ”€â”€ data-model.md    # Entity definitions
â”‚       â”œâ”€â”€ quickstart.md    # 3-minute quick start guide
â”‚       â”œâ”€â”€ contracts/       # API contracts
â”‚       â”‚   â””â”€â”€ nextjs-production-gen.yaml
â”‚       â”œâ”€â”€ VALIDATION-RESULTS.md      # Phase 1 validation (13/13 checks)
â”‚       â”œâ”€â”€ GOOSE-COMPATIBILITY.md     # Phase 2 cross-agent testing (100%)
â”‚       â”œâ”€â”€ SKILLS-PROGRESS.md         # Skills development progress
â”‚       â”œâ”€â”€ SKILLS-LIBRARY-README.md   # README for skills-library repo
â”‚       â”œâ”€â”€ SKILLS-ARCHITECTURE.md     # Architecture documentation
â”‚       â””â”€â”€ FINAL-SUMMARY.md           # Project summary
â”œâ”€â”€ history/                 # Prompt History Records (PHRs)
â”‚   â””â”€â”€ prompts/
â”‚       â””â”€â”€ 002-production-grade-rebuild/
â”‚           â”œâ”€â”€ 0001-skills-driven-production-rebuild-plan.plan.prompt.md
â”‚           â””â”€â”€ 0002-skills-driven-tasks-generation.tasks.prompt.md
â”œâ”€â”€ AGENTS.md                # This file (agent guidance)
â”œâ”€â”€ CLAUDE.md                # Pointer to AGENTS.md
â””â”€â”€ README.md                # Project overview
```

**Note**: Backend services, frontend application, and infrastructure are planned for future implementation. Current focus is on Skills development and validation.

## Common Tasks

### Generate Frontend Application

**Using Claude Code**:
```bash
# Invoke Skill directly
claude "Use nextjs-production-gen to generate a production frontend"
```

**Using Goose** (or any agent):
```bash
# Run scripts directly
python3 .claude/skills/nextjs-production-gen/scripts/check_prereqs.py
python3 .claude/skills/nextjs-production-gen/scripts/generate_complete_app.py \
  --design-system design-system.json \
  --output frontend/
python3 .claude/skills/nextjs-production-gen/scripts/verify_generation.py frontend/
```

### Validate Generated Application

```bash
# Run validation script
python3 .claude/skills/nextjs-production-gen/scripts/verify_generation.py frontend/

# Expected: 13/13 checks passed
```

### Install and Run Generated Application

```bash
cd frontend
npm install
npm run dev
# Open http://localhost:3000
```

## Hackathon Submission Checklist

**Development Process** (Single Repository):
- [ ] All Skills created in `.claude/skills/` in THIS EmberLearn repository
- [ ] All application code built using those Skills
- [ ] Commit history shows agentic workflow throughout

**Repository 1: skills-library** (Created at Submission):
- [ ] Create by copying `.claude/skills/` from EmberLearn repository
- [ ] Minimum 7 skills with SKILL.md + scripts/ + REFERENCE.md
- [ ] Each skill tested with Claude Code AND Goose
- [ ] README.md documents skill usage, installation (copy to ~/.claude/skills/), and development process
- [ ] Skills demonstrate autonomous execution (single prompt â†’ deployment)
- [ ] Token efficiency documented (before/after measurements)
- [ ] Submit as Repository 1 to hackathon form

**Repository 2: EmberLearn (this repository)**
- [ ] Contains BOTH `.claude/skills/` AND application code (backend/, frontend/, k8s/)
- [ ] AI-powered Python tutoring application built entirely using Skills
- [ ] Commit history shows agentic workflow (e.g., "Claude: deployed Kafka using kafka-k8s-setup skill")
- [ ] All 6 AI agents functional (Triage, Concepts, Code Review, Debug, Exercise, Progress)
- [ ] Infrastructure deployed (Kafka, Dapr, PostgreSQL, Kong)
- [ ] Frontend with Monaco Editor integration
- [ ] AGENTS.md present and comprehensive
- [ ] Documentation via Docusaurus
- [ ] Submit as Repository 2 to hackathon form

**Evaluation Criteria** (100 points):
- Skills Autonomy: 15%
- Token Efficiency: 10%
- Cross-Agent Compatibility: 5%
- Architecture: 20%
- MCP Integration: 10%
- Documentation: 10%
- Spec-Kit Plus Usage: 15%
- EmberLearn Completion: 15%

## Key Reminders

ğŸ¯ **Primary Focus**: Skills are the product. Every capability must be a reusable, autonomous Skill.

âš¡ **Token Efficiency**: Always use Skills + Scripts pattern. Never load MCP tools into context.

ğŸ”„ **Cross-Agent**: Test every Skill on both Claude Code AND Goose. AAIF standard compliance is mandatory.

ğŸ¤– **Autonomous Execution**: Single prompt â†’ complete deployment. Zero manual intervention.

ğŸ“‹ **Documentation**: PHR for every user prompt. ADR suggestions for significant decisions.

ğŸ—ï¸ **Cloud-Native**: Event-driven (Kafka), Dapr sidecars, stateless services, K8s patterns.

ğŸ” **Security**: JWT tokens, Kubernetes Secrets, no hardcoded credentials, PII tokenization.

## Constitution Reference

For complete project principles, see `.specify/memory/constitution.md` (v1.0.0).

Core principles:
1. Skills Are The Product
2. Token Efficiency First
3. Cross-Agent Compatibility
4. Autonomous Execution
5. Cloud-Native Architecture
6. MCP Code Execution Pattern
7. Test-Driven Development
8. Spec-Driven Development

---

**Submission Form**: https://forms.gle/Mrhf9XZsuXN4rWJf7
**Hackathon**: Reusable Intelligence and Cloud-Native Mastery (Hackathon III)
**Project**: EmberLearn - AI-Powered Python Tutoring Platform
