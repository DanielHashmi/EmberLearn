# EmberLearn (Hackathon III) Constitution

<!--
SYNC IMPACT REPORT - 2026-01-05

Version Change: 1.0.0 → 1.0.1 (Naming clarification - PATCH)

Changes in v1.0.1:
- Renamed "LearnFlow" to "EmberLearn" throughout (application name consistency)
- Clarified repository structure: skills-library (separate) + EmberLearn (this repo)
- Updated evaluation criteria: "LearnFlow Completion" → "EmberLearn Completion"
- No principle changes, only naming consistency (PATCH version bump)

Version History:
- v1.0.0 (2026-01-05): Initial constitution for Hackathon III project

Principles Defined:
- I. Skills Are The Product (NEW)
- II. Token Efficiency First (NEW)
- III. Cross-Agent Compatibility (NEW)
- IV. Autonomous Execution (NEW)
- V. Cloud-Native Architecture (NEW)
- VI. MCP Code Execution Pattern (NEW)
- VII. Test-Driven Development (NEW)
- VIII. Spec-Driven Development (NEW)

Sections Added:
- Core Principles (8 principles)
- Skills Development Standards
- Architecture Requirements
- Governance

Templates Status:
✅ plan-template.md - Constitution Check section compatible
✅ spec-template.md - Requirements align with principles
✅ tasks-template.md - Task organization supports skill-driven development

Follow-up TODOs:
- None - all placeholders filled

Rationale:
This is the initial constitution for Hackathon III: Reusable Intelligence and Cloud-Native Mastery.
Focus on Skills with MCP Code Execution as the primary deliverable, with EmberLearn as the
demonstration application built autonomously via these skills.

Repository Structure:
- skills-library: Separate repository containing 7+ reusable Skills
- EmberLearn: This repository - the AI-powered Python tutoring platform application
-->

## Core Principles

### I. Skills Are The Product

**The Skill is the product, not the application code.**

- Every capability MUST be implemented as a reusable Skill in `.claude/skills/`
- Skills MUST enable autonomous execution: single prompt → complete deployment
- Skills MUST be tested with both Claude Code AND Goose for cross-agent compatibility
- Application code (EmberLearn) MUST be generated by AI agents using these Skills
- Commit messages MUST reflect agentic workflow: "Claude: implemented X using Y skill"

**Rationale**: Judges evaluate the development process and test Skills for autonomous execution. The goal is teaching AI agents to build systems, not building systems manually.

### II. Token Efficiency First

**Optimize for minimal context window consumption.**

- MUST use Skills + Scripts pattern: `SKILL.md` (~100 tokens) → `scripts/*.py` (0 tokens, executed) → minimal result
- MUST NOT load MCP tool definitions into context at startup
- Scripts MUST execute outside context; only final results enter context
- `REFERENCE.md` MUST be loaded on-demand only, never proactively
- Target: 80-98% token reduction vs direct MCP integration

**Rationale**: Direct MCP integration consumes 50,000+ tokens before conversation starts. Skills + Scripts achieve 98.7% reduction (150k → 2k tokens demonstrated by Anthropic).

### III. Cross-Agent Compatibility

**Skills MUST work across Claude Code, Goose, and OpenAI Codex.**

- MUST use AAIF open standard format (SKILL.md with YAML frontmatter)
- MUST place skills in `.claude/skills/` (readable by all agents)
- MUST NOT use agent-specific features without fallbacks
- MUST test every Skill on both Claude Code AND Goose before submission
- Skills MUST use universal tools (Bash, Python, kubectl, helm) not proprietary APIs

**Rationale**: Industry convergence on Skills format (December 2025). Skills written once should work everywhere. Hackathon requires demonstrating cross-agent compatibility.

### IV. Autonomous Execution

**Single prompt to complete deployment with zero manual intervention.**

- Skills MUST include validation scripts that verify success
- Skills MUST handle prerequisite checks automatically
- Skills MUST provide clear error messages with remediation steps
- Skills MUST be idempotent (re-running should be safe)
- Skills MUST include automated rollback for failures where applicable

**Rationale**: Evaluation criterion "Skills Autonomy" (15% weight) measures single-prompt-to-deployment capability. Manual intervention reduces score.

### V. Cloud-Native Architecture

**Follow cloud-native, event-driven, stateless patterns.**

- MUST use event-driven communication (Kafka pub/sub) between services
- MUST use Dapr sidecar pattern for state, pub/sub, and service invocation
- Services MUST be stateless; state MUST reside in external stores (Neon PostgreSQL)
- MUST containerize all services with proper health checks
- MUST design for horizontal scalability (no single points of failure)
- MUST use Kubernetes-native patterns (ConfigMaps, Secrets, Services, Ingresses)

**Rationale**: EmberLearn is a cloud-native microservices application. Proper patterns ensure scalability, resilience, and maintainability.

### VI. MCP Code Execution Pattern

**Wrap MCP servers in executable scripts, not direct tool calls.**

- MUST follow structure: `.claude/skills/<skill-name>/` with `SKILL.md`, `scripts/`, `REFERENCE.md`
- `SKILL.md` MUST contain instructions only (~100 tokens, no implementation)
- `scripts/` MUST contain all executable code (deploy, verify, helpers)
- `REFERENCE.md` MUST contain deep documentation loaded only when needed
- MCP servers MUST be accessed via scripts, not loaded into agent context

**Rationale**: This is the core innovation of Hackathon III. Demonstrates understanding and application of Anthropic's MCP Code Execution pattern.

### VII. Test-Driven Development

**Tests written first, must fail, then implement.**

- Skills MUST include validation/verification scripts
- Application code SHOULD follow Red-Green-Refactor cycle when appropriate
- Integration tests MUST verify cross-service communication (Kafka, Dapr)
- Contract tests MUST validate API contracts between services
- End-to-end tests MUST validate complete user journeys

**Rationale**: Ensures quality and correctness. Skills validation is critical for autonomous execution scoring.

### VIII. Spec-Driven Development

**High-level specs translate to agentic instructions.**

- MUST use Spec-Kit Plus framework (`.specify/` templates)
- MUST create: Constitution → Spec → Plan → Tasks progression
- Specs MUST be technology-agnostic, focusing on outcomes
- Plans MUST document architectural decisions with rationale
- Tasks MUST be testable, dependency-ordered, and independently completable
- ADRs MUST be created for architecturally significant decisions

**Rationale**: Evaluation criterion "Spec-Kit Plus Usage" (15% weight). Demonstrates systematic approach to agent-driven development.

## Skills Development Standards

### SKILL.md Format (AAIF Standard)

Every Skill MUST include:

```yaml
---
name: skill-identifier                    # lowercase-with-hyphens, max 64 chars
description: What this does and when to use it  # max 1024 chars, semantic matching
allowed-tools: Tool1, Tool2               # Optional: restrict tool access
model: claude-sonnet-4-20250514           # Optional: override model
---

# Skill Display Name

## When to Use
- [Trigger condition 1]
- [Trigger condition 2]

## Instructions
1. [Step-by-step guidance]
2. [Clear, actionable instructions]

## Validation
- [ ] [Success criterion 1]
- [ ] [Success criterion 2]

See [REFERENCE.md](./REFERENCE.md) for detailed documentation.
```

### Required Skills (Minimum)

1. **agents-md-gen**: Generate AGENTS.md files for repositories
2. **kafka-k8s-setup**: Deploy Kafka on Kubernetes (Helm + verify)
3. **postgres-k8s-setup**: Deploy PostgreSQL on Kubernetes (migrations + verify)
4. **fastapi-dapr-agent**: Create FastAPI + Dapr + OpenAI Agent microservices
5. **mcp-code-execution**: Implement MCP with code execution pattern
6. **nextjs-k8s-deploy**: Deploy Next.js + Monaco Editor to Kubernetes
7. **docusaurus-deploy**: Deploy documentation site via Skill

### Skill Quality Standards

- SKILL.md MUST be under 500 lines (use REFERENCE.md for details)
- Description MUST be keyword-rich for semantic matching
- Scripts MUST be executable without modification
- Scripts MUST validate prerequisites before execution
- Scripts MUST return structured, parseable output
- REFERENCE.md MUST document configuration options, troubleshooting, examples

## Architecture Requirements

### EmberLearn Application Stack

| Layer | Technology | Requirements |
|-------|------------|--------------|
| **Frontend** | Next.js 15+ + Monaco Editor | SSR compatible, dynamic imports, responsive UI |
| **Auth** | Better Auth or NextAuth.js | JWT tokens, session management, secure cookies |
| **Backend** | FastAPI 0.110+ + OpenAI Agents SDK | Async I/O, OpenAPI docs, Pydantic validation |
| **Service Mesh** | Dapr 1.13+ | State management, pub/sub, service invocation |
| **Messaging** | Kafka 3.6+ (Bitnami Helm) | Topics: `learning.*`, `code.*`, `exercise.*`, `struggle.*` |
| **Database** | Neon PostgreSQL | Serverless, migrations via Alembic, connection pooling |
| **API Gateway** | Kong 3.5+ | JWT plugin, rate limiting, request routing |
| **Orchestration** | Kubernetes 1.28+ (Minikube) | 4 CPUs, 8GB RAM minimum for development |
| **CI/CD** | GitHub Actions + Argo CD | GitOps workflow, auto-sync, rollback capability |
| **Documentation** | Docusaurus 3.0+ | Auto-generated from code, search enabled |

### AI Agent Architecture (OpenAI Agents SDK)

EmberLearn MUST implement 6 specialized AI agents:

1. **Triage Agent**: Route queries to specialists based on intent
2. **Concepts Agent**: Explain Python concepts with adaptive examples
3. **Code Review Agent**: Analyze code for correctness, style (PEP 8), efficiency
4. **Debug Agent**: Parse errors, identify root causes, provide hints
5. **Exercise Agent**: Generate and auto-grade coding challenges
6. **Progress Agent**: Track mastery scores across modules

**Agent Implementation Pattern**:
- Each agent MUST be a FastAPI service
- Each service MUST have a Dapr sidecar
- Agents MUST communicate via Kafka pub/sub
- Agents MUST store state in Neon PostgreSQL via Dapr state API
- Agents MUST use OpenAI Agents SDK with structured tools
- Agents MUST publish events for all significant actions

### Mastery Calculation Formula

**Topic Mastery** = weighted average:
- Exercise completion: 40%
- Quiz scores: 30%
- Code quality ratings: 20%
- Consistency (streak): 10%

**Mastery Levels**:
- 0-40% → Beginner (Red)
- 41-70% → Learning (Yellow)
- 71-90% → Proficient (Green)
- 91-100% → Mastered (Blue)

### Struggle Detection Triggers

System MUST alert teachers when:
- Same error type 3+ times
- Stuck on exercise > 10 minutes
- Quiz score < 50%
- Student says "I don't understand" or "I'm stuck"
- 5+ failed code executions in a row

### Code Execution Sandbox

**Security Requirements**:
- Timeout: 5 seconds maximum
- Memory limit: 50MB
- No filesystem access (except temp directory)
- No network access
- Allowed imports: Python standard library only (MVP scope)

## Security Standards

### Authentication & Authorization

- MUST use JWT tokens with RS256 signing
- Tokens MUST expire within 24 hours
- Refresh tokens MUST be stored securely (HTTP-only cookies)
- API Gateway (Kong) MUST validate all tokens
- MUST implement role-based access control (Student, Teacher, Admin)

### Secrets Management

- MUST NEVER hardcode secrets in code or Skills
- MUST use Kubernetes Secrets for sensitive data
- MUST use environment variables for configuration
- MUST document required secrets in `.env.example`
- MUST use `.gitignore` to exclude `.env` files

### Data Privacy

- MUST tokenize PII before sending to AI models when possible
- MUST log only necessary data (no passwords, tokens, PII in logs)
- MUST use TLS for all external communication
- MUST implement audit logging for sensitive operations

## Governance

### Amendment Process

1. Propose change with rationale and impact analysis
2. Update constitution and increment version according to semantic versioning:
   - **MAJOR**: Backward-incompatible governance/principle changes
   - **MINOR**: New principles or materially expanded guidance
   - **PATCH**: Clarifications, wording fixes, non-semantic refinements
3. Update all dependent templates (plan, spec, tasks)
4. Document in Sync Impact Report (HTML comment at top of constitution)
5. Commit with message: `docs: amend constitution to vX.Y.Z (<summary>)`

### Compliance Review

- All Specs, Plans, and Tasks MUST reference this constitution
- Plan template MUST include "Constitution Check" gate
- PRs SHOULD be reviewed for principle compliance
- Complexity violations MUST be justified in "Complexity Tracking" section

### Hackathon Submission Requirements

**Development Workflow** (Single Repository During Development):
- All Skills are created in THIS repository at `.claude/skills/` for Claude Code to discover and use them
- All application code (EmberLearn) is built in THIS repository using those Skills
- At submission time, Skills are COPIED to create the separate skills-library repository

**Repository 1: skills-library** (Created at Submission)
- Created by copying `.claude/skills/` from EmberLearn repository
- Structure: `.claude/skills/<skill-name>/SKILL.md` + `scripts/` + `REFERENCE.md`
- Minimum 7 skills (agents-md-gen, kafka-k8s-setup, postgres-k8s-setup, fastapi-dapr-agent, mcp-code-execution, nextjs-k8s-deploy, docusaurus-deploy)
- Each skill MUST be tested with Claude Code AND Goose
- README.md MUST document skill usage, installation instructions (copying to user's ~/.claude/skills/), and development process
- Submit to hackathon form as separate repository

**Repository 2: EmberLearn (this repository)**
- Contains BOTH Skills (`.claude/skills/`) AND application code during development
- Complete AI-powered Python tutoring application built using Skills
- Commit history MUST show agentic workflow (e.g., "Claude: deployed Kafka using kafka-k8s-setup skill")
- All 6 AI agents MUST be functional (Triage, Concepts, Code Review, Debug, Exercise, Progress)
- Kafka, Dapr, PostgreSQL, Kong MUST be deployed
- Frontend MUST include Monaco Editor integration
- AGENTS.md MUST be present and comprehensive
- Submit to hackathon form with `.claude/skills/` still present to show Skills were used

**Submission Form**: https://forms.gle/Mrhf9XZsuXN4rWJf7

### Evaluation Criteria

| Criterion | Weight | Gold Standard |
|-----------|--------|---------------|
| Skills Autonomy | 15% | Single prompt → Running deployment, zero manual intervention |
| Token Efficiency | 10% | Skills use scripts, MCP wrapped efficiently, 80%+ reduction |
| Cross-Agent Compatibility | 5% | Same skill works on Claude Code AND Goose |
| Architecture | 20% | Correct Dapr, Kafka, stateless patterns |
| MCP Integration | 10% | Rich context enables AI debugging and expansion |
| Documentation | 10% | Comprehensive Docusaurus site via Skills |
| Spec-Kit Plus Usage | 15% | High-level specs → Agentic instructions |
| EmberLearn Completion | 15% | Full application built via skills |

**Total**: 100%

**Version**: 1.0.1 | **Ratified**: 2026-01-05 | **Last Amended**: 2026-01-05
