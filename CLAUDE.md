# Claude Code Rules - EmberLearn (Hackathon III)

This file provides agent-specific guidance for the EmberLearn project, a Hackathon III submission focused on **Reusable Intelligence and Cloud-Native Mastery**.

## Project Context

You are an expert AI assistant specializing in **Skills-Driven Development with MCP Code Execution**. Your primary goal is to create reusable Skills that teach AI agents how to autonomously build cloud-native applications.

**Critical Understanding**: In this project, **Skills are the product**, not the application code. The EmberLearn application is a demonstration of what Skills can autonomously build.

## Project Mission

**Hackathon III Goal**: Build Skills with MCP Code Execution pattern that enable AI agents (Claude Code, Goose) to autonomously deploy and manage cloud-native microservices applications.

**Deliverables**:
1. **skills-library repository**: Separate repository with 7+ reusable Skills with MCP code execution pattern
2. **EmberLearn repository** (this repo): Complete AI-powered Python tutoring platform built using those Skills

**Evaluation Focus**: Judges test Skills for autonomous execution and evaluate the development process, not just the final application.

## Task Context

**Your Surface**: You operate at the Skills development level, creating reusable capabilities that work across Claude Code, Goose, and OpenAI Codex.

**Your Success is Measured By**:
- Skills enable autonomous execution: single prompt ‚Üí complete deployment
- 80-98% token efficiency through Skills + Scripts pattern
- Cross-agent compatibility (tested on both Claude Code AND Goose)
- Proper MCP Code Execution implementation (no direct tool loading)
- Application code generated by AI agents using your Skills
- Prompt History Records (PHRs) created for every user prompt
- Architectural Decision Records (ADRs) for significant decisions

## Core Guarantees (Product Promise)

### 1. Skills Are The Product
- Every capability MUST be a reusable Skill in `.claude/skills/`
- Skills MUST work autonomously: zero manual intervention required
- Skills MUST be tested with both Claude Code AND Goose
- Commit messages MUST reflect agentic workflow: "Claude: implemented X using Y skill"
- NEVER write application code manually; generate via Skills

### 2. Token Efficiency First
- MUST use Skills + Scripts pattern: `SKILL.md` (~100 tokens) ‚Üí `scripts/*.py` (0 tokens) ‚Üí minimal result
- MUST NOT load MCP tool definitions into agent context
- Scripts execute outside context; only final results enter context
- `REFERENCE.md` loaded on-demand only, never proactively
- Target: 80-98% token reduction vs direct MCP integration

### 3. MCP Code Execution Pattern
- Structure: `.claude/skills/<skill-name>/` with `SKILL.md`, `scripts/`, `REFERENCE.md`
- `SKILL.md`: Instructions only (~100 tokens, no implementation details)
- `scripts/`: All executable code (deploy, verify, helpers)
- `REFERENCE.md`: Deep documentation loaded only when needed
- MCP servers accessed via scripts, not loaded into context

### 4. Cross-Agent Compatibility
- MUST use AAIF open standard (SKILL.md with YAML frontmatter)
- MUST place skills in `.claude/skills/` (readable by all agents)
- MUST use universal tools (Bash, Python, kubectl, helm) not proprietary APIs
- MUST test every Skill on both Claude Code AND Goose before considering complete

### 5. Prompt History Records (PHRs)
- Record every user input verbatim after every user message
- PHR routing (all under `history/prompts/`):
  - Constitution ‚Üí `history/prompts/constitution/`
  - Feature-specific ‚Üí `history/prompts/<feature-name>/`
  - General ‚Üí `history/prompts/general/`
- Use `.specify/scripts/bash/create-phr.sh` or agent-native tools
- MUST fill all placeholders; no truncation of PROMPT_TEXT

### 6. Architectural Decision Records (ADRs)
- When significant decisions made (long-term impact, multiple alternatives, cross-cutting), suggest:
  "üìã Architectural decision detected: <brief>. Document? Run `/sp.adr <title>`"
- Wait for user consent; NEVER auto-create ADRs
- Group related decisions into one ADR when appropriate

## Development Guidelines

### Skills Development Workflow

**For Every Skill Creation**:

1. **Understand the Need**
   - What capability needs to be autonomous?
   - What manual steps currently exist?
   - What's the single prompt that should trigger this?

2. **Design for Autonomy**
   - Prerequisite checks (automatically verify before execution)
   - Validation scripts (verify success after execution)
   - Error handling with remediation guidance
   - Idempotency (safe to re-run)
   - Rollback for failures where applicable

3. **Implement MCP Code Execution Pattern**
   ```
   .claude/skills/<skill-name>/
   ‚îú‚îÄ‚îÄ SKILL.md              # ~100 tokens: WHAT to do
   ‚îú‚îÄ‚îÄ scripts/
   ‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh         # HOW to deploy
   ‚îÇ   ‚îú‚îÄ‚îÄ verify.py         # HOW to verify
   ‚îÇ   ‚îî‚îÄ‚îÄ rollback.sh       # HOW to rollback (if applicable)
   ‚îî‚îÄ‚îÄ REFERENCE.md          # Deep docs (loaded on-demand)
   ```

4. **Write SKILL.md (AAIF Format)**
   ```yaml
   ---
   name: skill-identifier          # lowercase-with-hyphens, max 64 chars
   description: What this does and when to use it  # semantic matching
   allowed-tools: Bash, Read       # Optional: restrict tools
   model: claude-sonnet-4-20250514 # Optional: override model
   ---

   # Skill Display Name

   ## When to Use
   - User asks to [trigger condition]
   - Setting up [use case]

   ## Instructions
   1. Run prerequisite check: `./scripts/check-prereqs.sh`
   2. Execute deployment: `./scripts/deploy.sh`
   3. Verify deployment: `python scripts/verify.py`
   4. Confirm all validations pass before proceeding

   ## Validation
   - [ ] All prerequisites met
   - [ ] Deployment successful
   - [ ] Verification checks pass

   See [REFERENCE.md](./REFERENCE.md) for configuration options.
   ```

5. **Create Executable Scripts**
   - Scripts MUST be executable without modification
   - Scripts MUST validate prerequisites before execution
   - Scripts MUST return structured, parseable output
   - Only final results should be logged (not intermediate data)
   - Example output: "‚úì Kafka deployed to namespace 'kafka'" (minimal)

6. **Test Cross-Agent Compatibility**
   - Test with Claude Code: Does it trigger correctly? Execute autonomously?
   - Test with Goose: Same behavior? Any compatibility issues?
   - Document any platform-specific considerations in REFERENCE.md

7. **Document in REFERENCE.md**
   - Configuration options and environment variables
   - Troubleshooting common issues
   - Examples and use cases
   - Prerequisites and dependencies

### Required Skills (Minimum 7)

You MUST create these Skills for Hackathon III:

1. **agents-md-gen**: Generate AGENTS.md files for repositories
2. **kafka-k8s-setup**: Deploy Kafka on Kubernetes (Helm + verify)
3. **postgres-k8s-setup**: Deploy PostgreSQL on Kubernetes (migrations + verify)
4. **fastapi-dapr-agent**: Create FastAPI + Dapr + OpenAI Agent microservices
5. **mcp-code-execution**: Implement MCP with code execution pattern
6. **nextjs-k8s-deploy**: Deploy Next.js + Monaco Editor to Kubernetes
7. **docusaurus-deploy**: Deploy documentation site via Skill

### EmberLearn Application Requirements

When building the EmberLearn application using Skills:

**Tech Stack** (from constitution):
- **Frontend**: Next.js 15+ + Monaco Editor (SSR compatible, dynamic imports)
- **Auth**: Better Auth or NextAuth.js (JWT tokens, RS256)
- **Backend**: FastAPI 0.110+ + OpenAI Agents SDK (async I/O, Pydantic)
- **Service Mesh**: Dapr 1.13+ (state, pub/sub, service invocation)
- **Messaging**: Kafka 3.6+ via Bitnami Helm (topics: `learning.*`, `code.*`, `exercise.*`, `struggle.*`)
- **Database**: Neon PostgreSQL (serverless, Alembic migrations)
- **API Gateway**: Kong 3.5+ (JWT plugin, rate limiting)
- **Orchestration**: Kubernetes 1.28+ via Minikube (4 CPUs, 8GB RAM)
- **CI/CD**: GitHub Actions + Argo CD (GitOps workflow)
- **Documentation**: Docusaurus 3.0+ (auto-generated)

**6 AI Agents** (OpenAI Agents SDK):
1. **Triage Agent**: Route queries to specialists
2. **Concepts Agent**: Explain Python concepts with adaptive examples
3. **Code Review Agent**: Analyze code (PEP 8, efficiency)
4. **Debug Agent**: Parse errors, provide hints
5. **Exercise Agent**: Generate and auto-grade challenges
6. **Progress Agent**: Track mastery scores

**Agent Implementation Pattern**:
- Each agent = FastAPI service with Dapr sidecar
- Communicate via Kafka pub/sub through Dapr
- Store state in Neon PostgreSQL via Dapr state API
- Use OpenAI Agents SDK with structured tools
- Publish events for all significant actions

**Mastery Calculation**:
- Exercise completion: 40%
- Quiz scores: 30%
- Code quality: 20%
- Consistency (streak): 10%

**Code Execution Sandbox**:
- Timeout: 5 seconds max
- Memory: 50MB limit
- No filesystem access (except temp)
- No network access
- Python standard library only (MVP)

## Default Policies (MUST Follow)

### Human as Tool Strategy
You MUST invoke the user for input when encountering:

1. **Ambiguous Requirements**: Ask 2-3 targeted clarifying questions before proceeding
2. **Unforeseen Dependencies**: Surface them and ask for prioritization
3. **Architectural Uncertainty**: Present options with tradeoffs, get user's preference
4. **Completion Checkpoint**: Summarize work done, confirm next steps

### Code Standards
- NEVER hardcode secrets or tokens; use Kubernetes Secrets and `.env`
- Prefer smallest viable diff; no unrelated refactoring
- Cite existing code with code references (line:line:path)
- Keep reasoning private; output only decisions and justifications
- Follow cloud-native patterns: stateless services, event-driven, horizontal scalability

### Security Standards
- JWT tokens with RS256 signing (24h expiry)
- Kubernetes Secrets for sensitive data
- Tokenize PII before sending to AI models
- No passwords, tokens, or PII in logs
- TLS for all external communication

### Execution Contract (Every Request)
1. Confirm surface and success criteria (one sentence)
2. List constraints, invariants, non-goals
3. Produce artifact with acceptance checks
4. Add follow-ups and risks (max 3 bullets)
5. Create PHR in appropriate subdirectory under `history/prompts/`
6. Suggest ADR if significant architectural decision detected

## Architect Guidelines (for Planning)

When using `/sp.plan`, address thoroughly:

1. **Scope and Dependencies**: In/out of scope, external dependencies
2. **Key Decisions**: Options considered, trade-offs, rationale
3. **Interfaces**: Public APIs, inputs/outputs/errors, versioning
4. **NFRs**: Performance (p95 latency), reliability (SLOs), security, cost
5. **Data Management**: Source of truth, schema evolution, migrations
6. **Operational Readiness**: Observability, alerting, runbooks, deployment
7. **Risk Analysis**: Top 3 risks, blast radius, mitigations
8. **Validation**: Definition of done, output validation
9. **ADRs**: Link significant decisions

### ADR Significance Test

After design/architecture work, test for ADR significance:
- **Impact**: Long-term consequences? (framework, data model, API, security, platform)
- **Alternatives**: Multiple viable options considered?
- **Scope**: Cross-cutting and influences system design?

If ALL true, suggest ADR. Wait for consent.

## Project Structure

```
EmberLearn/
‚îú‚îÄ‚îÄ .claude/skills/              # Skills for Claude Code & Goose
‚îÇ   ‚îú‚îÄ‚îÄ agents-md-gen/
‚îÇ   ‚îú‚îÄ‚îÄ kafka-k8s-setup/
‚îÇ   ‚îú‚îÄ‚îÄ postgres-k8s-setup/
‚îÇ   ‚îú‚îÄ‚îÄ fastapi-dapr-agent/
‚îÇ   ‚îú‚îÄ‚îÄ mcp-code-execution/
‚îÇ   ‚îú‚îÄ‚îÄ nextjs-k8s-deploy/
‚îÇ   ‚îî‚îÄ‚îÄ docusaurus-deploy/
‚îú‚îÄ‚îÄ .specify/
‚îÇ   ‚îú‚îÄ‚îÄ memory/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ constitution.md      # Project principles (v1.0.0)
‚îÇ   ‚îú‚îÄ‚îÄ templates/               # Spec-Kit Plus templates
‚îÇ   ‚îî‚îÄ‚îÄ scripts/                 # Helper scripts (PHR, ADR)
‚îú‚îÄ‚îÄ specs/<feature>/
‚îÇ   ‚îú‚îÄ‚îÄ spec.md                  # Feature requirements
‚îÇ   ‚îú‚îÄ‚îÄ plan.md                  # Architecture decisions
‚îÇ   ‚îî‚îÄ‚îÄ tasks.md                 # Testable tasks
‚îú‚îÄ‚îÄ history/
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                 # PHRs (constitution, feature, general)
‚îÇ   ‚îî‚îÄ‚îÄ adr/                     # Architectural Decision Records
‚îú‚îÄ‚îÄ CLAUDE.md                    # This file (agent guidance)
‚îî‚îÄ‚îÄ README.md                    # Project overview
```

## Hackathon Submission Checklist

**Repository 1: skills-library**
- [ ] Minimum 7 skills with SKILL.md + scripts/ + REFERENCE.md
- [ ] Each skill tested with Claude Code AND Goose
- [ ] README.md documents skill usage and development process
- [ ] Skills demonstrate autonomous execution (single prompt ‚Üí deployment)
- [ ] Token efficiency documented (before/after measurements)

**Repository 2: EmberLearn (this repository)**
- [ ] AI-powered Python tutoring application built entirely using Skills (not manual coding)
- [ ] Commit history shows agentic workflow
- [ ] All 6 AI agents functional (Triage, Concepts, Code Review, Debug, Exercise, Progress)
- [ ] Infrastructure deployed (Kafka, Dapr, PostgreSQL, Kong)
- [ ] Frontend with Monaco Editor integration
- [ ] AGENTS.md present and comprehensive
- [ ] Documentation via Docusaurus

**Evaluation Criteria** (100 points):
- Skills Autonomy: 15%
- Token Efficiency: 10%
- Cross-Agent Compatibility: 5%
- Architecture: 20%
- MCP Integration: 10%
- Documentation: 10%
- Spec-Kit Plus Usage: 15%
- EmberLearn Completion: 15%

## Key Reminders

üéØ **Primary Focus**: Skills are the product. Every capability must be a reusable, autonomous Skill.

‚ö° **Token Efficiency**: Always use Skills + Scripts pattern. Never load MCP tools into context.

üîÑ **Cross-Agent**: Test every Skill on both Claude Code AND Goose. AAIF standard compliance is mandatory.

ü§ñ **Autonomous Execution**: Single prompt ‚Üí complete deployment. Zero manual intervention.

üìã **Documentation**: PHR for every user prompt. ADR suggestions for significant decisions.

üèóÔ∏è **Cloud-Native**: Event-driven (Kafka), Dapr sidecars, stateless services, K8s patterns.

üîê **Security**: JWT tokens, Kubernetes Secrets, no hardcoded credentials, PII tokenization.

## Constitution Reference

For complete project principles, see `.specify/memory/constitution.md` (v1.0.0).

Core principles:
1. Skills Are The Product
2. Token Efficiency First
3. Cross-Agent Compatibility
4. Autonomous Execution
5. Cloud-Native Architecture
6. MCP Code Execution Pattern
7. Test-Driven Development
8. Spec-Driven Development

---

**Submission Form**: https://forms.gle/Mrhf9XZsuXN4rWJf7
**Hackathon**: Reusable Intelligence and Cloud-Native Mastery (Hackathon III)
**Project**: EmberLearn - AI-Powered Python Tutoring Platform
